from fastapi import FastAPI
from pydantic import BaseModel
from main import chat_loop  # âŒ æš‚æ—¶åˆ«ç›´æ¥å¯¼ mainï¼Œå¯èƒ½ä¼šæ­»å¾ªç¯ï¼Œæˆ‘ä»¬ç¨åæ‰‹åŠ¨æ¬é€»è¾‘
# æ­£ç¡®åšæ³•ï¼šå¯¼å…¥å·¥å…·å’Œé…ç½®
import config
from tools import tools_map, tools_schema
from openai import OpenAI
import json

app = FastAPI()

# 1. å®šä¹‰è¯·æ±‚çš„æ•°æ®æ ¼å¼ (Pydantic ç«‹åŠŸäº†ï¼)
class UserQuery(BaseModel):
    query: str

# 2. åˆå§‹åŒ–å®¢æˆ·ç«¯ (æ¬è¿ä¹‹å‰çš„ä»£ç )
client = OpenAI(api_key=config.API_KEY, base_url=config.BASE_URL)

@app.post("/chat")  # æ³¨æ„è¿™é‡Œå˜æˆäº† POSTï¼Œå› ä¸ºæˆ‘ä»¬è¦å‘æ•°æ®ç»™æœåŠ¡å™¨
def chat_endpoint(user_input: UserQuery):
    print(f"ğŸ“© æ”¶åˆ°ç”¨æˆ·è¯·æ±‚: {user_input.query}")
    
    # 1. æ„é€ æ¶ˆæ¯åˆ—è¡¨ (System Prompt + ç”¨æˆ·è¾“å…¥)
    messages = [
        {"role": "system", "content": config.SYSTEM_PROMPT},
        {"role": "user", "content": user_input.query}
    ]

    # 2. è°ƒç”¨ LLM (è¿˜æ˜¯ç†Ÿæ‚‰çš„é…æ–¹)
    # æ³¨æ„ï¼šä¸ºäº† API å“åº”é€Ÿåº¦ï¼Œè¿™é‡Œæˆ‘ä»¬æš‚æ—¶ä¸åŠ  Tool Calling çš„å¤æ‚å¾ªç¯ï¼Œå…ˆæµ‹è¯•å¯¹è¯
    response = client.chat.completions.create(
        model=config.MODEL_NAME,
        messages=messages,
        temperature=0.7
    )

    # 3. æå– AI çš„å›ç­”
    ai_content = response.choices[0].message.content
    
    # 4. è¿”å›ç»™ç”¨æˆ·
    return {
        "user": user_input.query,
        "ai_response": ai_content
    }
